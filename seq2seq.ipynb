{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 25\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "from BERT_WordEmbeddingsPipeline import embeddingsPipeline\n",
    "embedding_length = 768\n",
    "zero_embedding = [0 for i in range(embedding_length)]\n",
    "\n",
    "print(len(zero_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     tokenized_sents\n",
      "0  It never once occurred to me that the fumbling...\n",
      "1  Finding nothing else not even gold the Superin...\n",
      "2  Herbert West needed fresh bodies because his l...\n",
      "3  The farm like grounds extended back very deepl...\n",
      "4  His facial aspect too was remarkable for its m...\n",
      "(5635, 1)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('hpl.csv')\n",
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 200\n",
    "\n",
    "emb = []\n",
    "for i in data['tokenized_sents']:\n",
    "    e = embeddingsPipeline(i)\n",
    "    while(len(e) < max_words):\n",
    "        e.append(zero_embedding)\n",
    "    e = e[:200]\n",
    "    emb.append(e)\n",
    "    \n",
    "data['embeddings'] = emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_sents</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>[[1.0952692, -1.5751679, -2.1890771, -1.346773...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Finding nothing else not even gold the Superin...</td>\n",
       "      <td>[[-2.8680723, 1.028281, 0.8684244, -0.20770997...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Herbert West needed fresh bodies because his l...</td>\n",
       "      <td>[[1.1890708, 3.4342752, -1.1254256, 1.5733889,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The farm like grounds extended back very deepl...</td>\n",
       "      <td>[[-0.47942826, -1.4710225, -0.8196672, -0.9625...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>His facial aspect too was remarkable for its m...</td>\n",
       "      <td>[[-2.287135, -0.24020833, -1.8980496, -0.57004...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     tokenized_sents  \\\n",
       "0  It never once occurred to me that the fumbling...   \n",
       "1  Finding nothing else not even gold the Superin...   \n",
       "2  Herbert West needed fresh bodies because his l...   \n",
       "3  The farm like grounds extended back very deepl...   \n",
       "4  His facial aspect too was remarkable for its m...   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [[1.0952692, -1.5751679, -2.1890771, -1.346773...  \n",
       "1  [[-2.8680723, 1.028281, 0.8684244, -0.20770997...  \n",
       "2  [[1.1890708, 3.4342752, -1.1254256, 1.5733889,...  \n",
       "3  [[-0.47942826, -1.4710225, -0.8196672, -0.9625...  \n",
       "4  [[-2.287135, -0.24020833, -1.8980496, -0.57004...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4508, 2) (563, 2) (564, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, remaining_data = train_test_split(data, test_size=0.2, random_state=SEED)\n",
    "test_data, valid_data = train_test_split(remaining_data, test_size=0.5, random_state=SEED)\n",
    "\n",
    "print(train_data.shape, test_data.shape, valid_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, isCuda):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.isCuda = isCuda\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # initializing weights\n",
    "        nn.init.xavier_uniform(self.lstm.weight_ih_l0, gain=np.sqrt(2))\n",
    "        nn.init.xavier_uniform(self.lstm.weight_hh_l0, gain=np.sqrt(2))\n",
    "         \n",
    "    def forward(self, input):\n",
    "        encoded_input, hidden = self.lstm(input)\n",
    "        encoded_input = self.relu(encoded_input)\n",
    "        return encoded_input\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers, isCuda):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.isCuda = isCuda\n",
    "        self.lstm = nn.LSTM(hidden_size, output_size, num_layers, batch_first=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        nn.init.xavier_uniform(self.lstm.weight_ih_l0, gain=np.sqrt(2))\n",
    "        nn.init.xavier_uniform(self.lstm.weight_hh_l0, gain=np.sqrt(2))\n",
    "       \n",
    "    def forward(self, encoded_input):\n",
    "        decoded_output, hidden = self.lstm(encoded_input)\n",
    "        decoded_output = self.sigmoid(decoded_output)\n",
    "        return decoded_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_AE(nn.Module):\n",
    "    def __init__(self, encoder, decoder, isCuda):\n",
    "        super(LSTM_AE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.isCuda = isCuda\n",
    "        \n",
    "    def forward(self, input):\n",
    "        encoded_input = self.encoder(input)\n",
    "        decoded_output = self.decoder(encoded_input)\n",
    "        return decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chait\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  del sys.path[0]\n",
      "C:\\Users\\chait\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \n",
      "C:\\Users\\chait\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "C:\\Users\\chait\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    }
   ],
   "source": [
    "INPUT_SIZE = embedding_length\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 2\n",
    "OUTPUT_SIZE = embedding_length\n",
    "IS_CUDA = torch.cuda.is_available()\n",
    "\n",
    "enc = Encoder(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, IS_CUDA)\n",
    "dec = Decoder(HIDDEN_SIZE, OUTPUT_SIZE, NUM_LAYERS, IS_CUDA)\n",
    "model = LSTM_AE(enc, dec, IS_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4693    [[-4.7064943, -1.5040853, 1.2175621, 1.5480866...\n",
       "4226    [[3.3086557, 0.09898888, -3.8257747, 0.6116794...\n",
       "1723    [[2.2980692, 1.2661281, -0.25308305, 1.8103327...\n",
       "4944    [[1.086747, -0.052063126, -0.60388684, 0.19703...\n",
       "3783    [[-2.4465256, 3.5449834, -1.2721826, -1.445640...\n",
       "Name: embeddings, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['embeddings'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_AE(\n",
       "  (encoder): Encoder(\n",
       "    (lstm): LSTM(768, 512, num_layers=2, batch_first=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (lstm): LSTM(512, 768, num_layers=2, batch_first=True)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data, test_data, model, epochs):\n",
    "    \n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    print(\"Started Training ...\")\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_epoch_loss = 0\n",
    "        for text in train_data:\n",
    "            text = torch.Tensor(text)\n",
    "            \n",
    "            text = text.to(device)\n",
    "            text = text.view(max_words, 1, embedding_length)\n",
    "            output = model(text)\n",
    "            loss = criterion(text, output)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_epoch_loss += loss.item()\n",
    "            \n",
    "        test_epoch_loss = 0\n",
    "        for text in train_data:\n",
    "            text = torch.Tensor(text)\n",
    "            \n",
    "            text = text.to(device)\n",
    "            text = text.view(max_words, 1, embedding_length)\n",
    "            output = model(text)\n",
    "            loss = criterion(text, output)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            test_epoch_loss += loss.item()\n",
    "        \n",
    "        finish_time = time.time()\n",
    "        print(\"Epoch {}: Loss from Training data: {} and Loss from Test data: {} Time taken: {}\" \n",
    "              .format(epoch, train_epoch_loss/len(train_data), test_epoch_loss/len(test_data), finish_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Training ...\n",
      "Epoch 0: Loss from Training data: 1.0399610326784896 and Loss from Test data: 8.281609604985421 Time taken: 210.87269473075867\n",
      "Epoch 1: Loss from Training data: 1.0312922497171688 and Loss from Test data: 8.240919174590077 Time taken: 215.92697954177856\n",
      "Epoch 2: Loss from Training data: 1.0278757249386097 and Loss from Test data: 8.222859383873363 Time taken: 219.4379162788391\n",
      "Epoch 3: Loss from Training data: 1.0261627643665807 and Loss from Test data: 8.211620027335034 Time taken: 221.91909527778625\n",
      "Epoch 4: Loss from Training data: 1.0250699079488903 and Loss from Test data: 8.204774286538415 Time taken: 222.91722321510315\n",
      "Epoch 5: Loss from Training data: 1.0243461487390029 and Loss from Test data: 8.199733608728192 Time taken: 225.33042907714844\n",
      "Epoch 6: Loss from Training data: 1.0238147356548242 and Loss from Test data: 8.19615974630598 Time taken: 223.68233108520508\n",
      "Epoch 7: Loss from Training data: 1.0234338087463601 and Loss from Test data: 8.193407905832277 Time taken: 224.20715999603271\n",
      "Epoch 8: Loss from Training data: 1.0231069335028093 and Loss from Test data: 8.190906170153491 Time taken: 227.35839676856995\n",
      "Epoch 9: Loss from Training data: 1.022809349450001 and Loss from Test data: 8.188761948162878 Time taken: 224.8929307460785\n",
      "Epoch 10: Loss from Training data: 1.0225622577339861 and Loss from Test data: 8.186953387929533 Time taken: 226.05227518081665\n",
      "Epoch 11: Loss from Training data: 1.0223712425417835 and Loss from Test data: 8.185579869717518 Time taken: 227.92101526260376\n",
      "Epoch 12: Loss from Training data: 1.0221988643555115 and Loss from Test data: 8.184062903823056 Time taken: 225.94636631011963\n",
      "Epoch 13: Loss from Training data: 1.0220234168110003 and Loss from Test data: 8.182813827669642 Time taken: 228.25220489501953\n",
      "Epoch 14: Loss from Training data: 1.021874683977707 and Loss from Test data: 8.181758440620735 Time taken: 304.2109098434448\n"
     ]
    }
   ],
   "source": [
    "train(train_data['embeddings'], test_data['embeddings'], model, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
